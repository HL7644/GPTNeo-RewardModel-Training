{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#library for data processing\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from itertools import combinations\n",
    "import io\n",
    "import json\n",
    "import os\n",
    "import ast\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import transformers\n",
    "import accelerate\n",
    "from transformers import GPTNeoForCausalLM, GPT2Tokenizer, AdamW, get_scheduler, GPTNeoModel, GPT2LMHeadModel\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "#from torchrl.data import PrioritizedReplayBuffer, ReplayBuffer\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "import pickle\n",
    "import gym\n",
    "\n",
    "base_dir=\".\"\n",
    "\n",
    "accelerator=accelerate.Accelerator()\n",
    "device=accelerator.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing all datasets\n",
    "from dataset_classes.hf_dataset import HFDataset\n",
    "from dataset_classes.state_action_dataset import StateActionDataset\n",
    "from dataset_classes.ep_steps_dataset import EpStepsDataset\n",
    "from dataset_classes.eli5_and_hf_dataset import ELI5andHFDataset\n",
    "from dataset_classes.chosen_dataset import ChosenDataset\n",
    "from dataset_classes.eli5_dataset import ELI5Dataset\n",
    "from dataset_classes.instruct_dataset import InstructDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#length threshold to reduce computational load during training\n",
    "len_thresh=500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#125M model\n",
    "class SFTModel(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(SFTModel, self).__init__()\n",
    "    self.model=GPTNeoForCausalLM.from_pretrained('EleutherAI/gpt-neo-125M').to(device)\n",
    "    self.tokenizer = GPT2Tokenizer.from_pretrained('EleutherAI/gpt-neo-125M', bos_token='<|endoftext|>', eos_token='<|endoftext|>', padding_side=\"left\")\n",
    "    self.tokenizer.pad_token='<|endoftext|>'\n",
    "  \n",
    "  def generate(self, batch_data, max_gen_length):\n",
    "    #get LM loss for batch data\n",
    "    tokenized=self.tokenizer(batch_data, return_tensors='pt', padding=True)\n",
    "    input_ids, attention_mask=tokenized.input_ids.to(device), tokenized.attention_mask.to(device)\n",
    "    #no need for scores.\n",
    "    generated=self.model.generate(input_ids, attention_mask=attention_mask, max_length=max_gen_length, pad_token_id=self.tokenizer.eos_token_id)\n",
    "    gen_sequences=generated[:,input_ids.size(1):]\n",
    "    gen_sentences=self.tokenizer.batch_decode(gen_sequences)\n",
    "    return gen_sequences, gen_sentences\n",
    "  \n",
    "  def forward(self, batch_data):\n",
    "    #get LM loss for batch data\n",
    "    tokenized=self.tokenizer(batch_data, return_tensors='pt', padding=True)\n",
    "    input_ids, attention_mask=tokenized.input_ids.to(device), tokenized.attention_mask.to(device)\n",
    "    output=self.model(input_ids, attention_mask=attention_mask, labels=input_ids)\n",
    "    loss=output.loss\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#10M model\n",
    "class SFTModel_10M(nn.Module):\n",
    "  def __init__(self, idx):\n",
    "    super(SFTModel_10M, self).__init__()\n",
    "    self.model=GPTNeoForCausalLM.from_pretrained('EleutherAI/gpt-neo-125M').to(device)\n",
    "    attention_layers=self.model.transformer.h\n",
    "    layer=attention_layers[idx]\n",
    "    self.model.transformer.h=nn.ModuleList([layer])\n",
    "    self.tokenizer = GPT2Tokenizer.from_pretrained('EleutherAI/gpt-neo-125M', bos_token='<|endoftext|>', eos_token='<|endoftext|>', padding_side=\"left\")\n",
    "    self.tokenizer.pad_token='<|endoftext|>'\n",
    "  \n",
    "  def generate(self, batch_data, max_gen_length):\n",
    "    #get LM loss for batch data\n",
    "    tokenized=self.tokenizer(batch_data, return_tensors='pt', padding=True)\n",
    "    input_ids, attention_mask=tokenized.input_ids.to(device), tokenized.attention_mask.to(device)\n",
    "    #no need for scores. & get the whole sequence: state+action\n",
    "    gen_sequences=self.model.generate(input_ids, attention_mask=attention_mask, max_length=max_gen_length, pad_token_id=self.tokenizer.eos_token_id)\n",
    "    gen_sentences=self.tokenizer.batch_decode(gen_sequences, skip_special_tokens=True)\n",
    "    return gen_sequences, gen_sentences\n",
    "  \n",
    "  def forward(self, batch_data):\n",
    "    #get LM loss for batch data\n",
    "    tokenized=self.tokenizer(batch_data, return_tensors='pt', padding=True)\n",
    "    input_ids, attention_mask=tokenized.input_ids.to(device), tokenized.attention_mask.to(device)\n",
    "    output=self.model(input_ids, attention_mask=attention_mask, labels=input_ids)\n",
    "    loss=output.loss\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTNeoRewardModel_10M(nn.Module):\n",
    "  def __init__(self, idx=0):\n",
    "    super(GPTNeoRewardModel_10M, self).__init__()\n",
    "    self.base=GPTNeoModel.from_pretrained('EleutherAI/gpt-neo-125M').to(device)\n",
    "    attention_layers=self.base.h\n",
    "    layer=attention_layers[idx]\n",
    "    self.base.h=nn.ModuleList([layer])\n",
    "    self.tokenizer = GPT2Tokenizer.from_pretrained('EleutherAI/gpt-neo-125M', bos_token='<|endoftext|>', eos_token='<|endoftext|>', padding_side=\"left\")\n",
    "    self.reward_head=nn.Linear(768, 1).to(device)\n",
    "    self.tokenizer.pad_token='<|endoftext|>'\n",
    "    \n",
    "  def forward(self, prompts):\n",
    "    #prompts include both state and action could be raw text of tensor\n",
    "    tokenized=self.tokenizer(prompts, return_tensors='pt', padding=True)\n",
    "    input_ids, attention_mask=tokenized.input_ids.to(device), tokenized.attention_mask.to(device)\n",
    "    last_hidden_states=self.base(input_ids, attention_mask=attention_mask).last_hidden_state\n",
    "    #use final hidden state's EOS embedding\n",
    "    eos_hidden_states=last_hidden_states[:,-1,:]\n",
    "    rewards=self.reward_head(eos_hidden_states)\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SFTTrainer():\n",
    "  def __init__(self, data, max_gen_length, reward_model_name, idx=0):\n",
    "    self.sft_model=SFTModel_10M(idx)\n",
    "    self.max_gen_length=max_gen_length\n",
    "\n",
    "    #use trained reward model as evaluator. (10M setting)\n",
    "    self.evaluator=GPTNeoRewardModel_10M()\n",
    "    self.evaluator.load_state_dict(torch.load(os.path.join(base_dir, 'models/modified_rm/{:s}.pt'.format(reward_model_name))))\n",
    "    self.evaluator.eval()\n",
    "\n",
    "    #data\n",
    "    self.train_data=data['train_data']\n",
    "    self.val_data=data['val_data']\n",
    "    self.test_data=data['test_data']\n",
    "\n",
    "    #train logs\n",
    "    self.train_logs={\n",
    "        'loss_history':[],\n",
    "        'val_score_history':[],\n",
    "        'best_val_score':0,\n",
    "        'best_val_score_epoch':0,\n",
    "        'test_score':0,\n",
    "        'best_sft_model': self.sft_model #initialize with default model\n",
    "    }\n",
    "  \n",
    "  def load_saved_model(self, model_name):\n",
    "    self.sft_model.load_state_dict(torch.load(os.path.join(base_dir, 'models/sft/{:s}.pt'.format(model_name))))\n",
    "    self.train_logs['best_sft_model'].load_state_dict(torch.load(os.path.join(base_dir, 'models/sft/{:s}.pt'.format(model_name))))\n",
    "    return\n",
    "  \n",
    "  def plot_train_logs(self):\n",
    "    loss_history=self.train_logs['loss_history']\n",
    "    iters=np.arange(0, len(loss_history), 1)\n",
    "    val_score_history=self.train_logs['val_score_history']\n",
    "    n_epochs=np.arange(1, len(val_score_history)+1, 1)\n",
    "    \n",
    "    plt.figure(figsize=(15,5))\n",
    "    #plot loss\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(iters, loss_history)\n",
    "    plt.title(\"Loss History\")\n",
    "    plt.xlabel(\"Iterations\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    #plot accuracy\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(n_epochs, val_score_history, \"r-\", label=\"Valid. Score\")\n",
    "    #mark best val acc location\n",
    "    plt.plot(self.train_logs['best_val_score_epoch'], self.train_logs['best_val_score'], \"go\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Reward Model Score\")\n",
    "    plt.title(\"Validation Score\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "  def test(self):\n",
    "    test_loader=torch.utils.data.DataLoader(self.test_data, batch_size=4, shuffle=True)\n",
    "    avg_score=0\n",
    "    #test with best model\n",
    "    best_model=self.train_logs['best_sft_model']\n",
    "    best_model.eval()\n",
    "    for iter in range(5):\n",
    "      avg_reward=0\n",
    "      for idx, test_data in enumerate(test_loader):\n",
    "        chosens, states, actions=test_data\n",
    "        _, gen_sentences=best_model.generate(states, self.max_gen_length)\n",
    "        rewards=self.evaluator(gen_sentences)\n",
    "        avg_reward=avg_reward+(torch.mean(rewards).item()-avg_reward)/(idx+1)\n",
    "      avg_score=avg_score+(avg_reward-avg_score)/(iter+1)\n",
    "    return avg_score\n",
    "  \n",
    "  def validate(self):\n",
    "    val_loader=torch.utils.data.DataLoader(self.val_data, batch_size=4, shuffle=True)\n",
    "    avg_score=0\n",
    "    self.sft_model.eval()\n",
    "    #validate with current model\n",
    "    for iter in range(5):\n",
    "      avg_reward=0\n",
    "      for idx, test_data in enumerate(val_loader):\n",
    "        chosens, states, actions=test_data\n",
    "        _, gen_sentences=self.sft_model.generate(states, self.max_gen_length)\n",
    "        rewards=self.evaluator(gen_sentences)\n",
    "        avg_reward=avg_reward+(torch.mean(rewards).item()-avg_reward)/(idx+1)\n",
    "      avg_score=avg_score+(avg_reward-avg_score)/(iter+1)\n",
    "    return avg_score\n",
    "\n",
    "  def train(self, num_epochs, batch_size, lr, reg):\n",
    "    train_loader=torch.utils.data.DataLoader(self.train_data, batch_size=batch_size, shuffle=True)\n",
    "    sft_optimizer=optim.AdamW(self.sft_model.parameters(), lr=lr, weight_decay=reg)\n",
    "\n",
    "    pbar = tqdm(desc=\"sft-training\", total=len(self.train_data)*num_epochs, leave=False)\n",
    "    for epoch in range(1, num_epochs+1):\n",
    "      for batch_idx, batch_data in enumerate(train_loader):\n",
    "        chosens, states, actions=batch_data\n",
    "        self.sft_model.train()\n",
    "        loss=self.sft_model(chosens)\n",
    "        self.train_logs['loss_history'].append(loss.item())\n",
    "\n",
    "        #update\n",
    "        sft_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        sft_optimizer.step()\n",
    "      \n",
    "        pbar.update(batch_size)\n",
    "\n",
    "      val_score=self.validate()\n",
    "      self.train_logs['val_score_history'].append(val_score)\n",
    "      if val_score>self.train_logs['best_val_score']:\n",
    "        self.train_logs['best_val_score']=val_score\n",
    "        self.train_logs['best_sft_model']=self.sft_model\n",
    "        self.train_logs['best_val_score_epoch']=epoch\n",
    "    pbar.close()\n",
    "    test_score=self.test()\n",
    "    self.train_logs['test_score']=test_score\n",
    "    return self.train_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SFTHyperparamTuner():\n",
    "  def __init__(self, data, max_gen_length, reward_model_name):\n",
    "    self.data=data\n",
    "    self.max_gen_length=max_gen_length\n",
    "    self.reward_model_name=reward_model_name\n",
    "\n",
    "    #log results\n",
    "    self.tuning_logs={\n",
    "        'logs': {},\n",
    "        'best_setting':[],\n",
    "        'best_test_score':0,\n",
    "        'best_sft_model':None\n",
    "    }\n",
    "  \n",
    "  def get_settings(self, num_epochs_list, batch_size_list, lr_list, reg_list):\n",
    "    settings=[]\n",
    "    for num_epochs in num_epochs_list:\n",
    "      for batch_size in batch_size_list:\n",
    "        for lr in lr_list:\n",
    "          for reg in reg_list:\n",
    "            setting=(num_epochs, batch_size, lr, reg)\n",
    "            settings.append(setting)\n",
    "    return settings\n",
    "  \n",
    "  def show_setting(self, setting):\n",
    "    print(\"Setting: num_epochs={:d}, batch_size={:d}, lr={:s}, reg={:s}\".format(setting[0], setting[1], str(setting[2]), str(setting[3])))\n",
    "  \n",
    "  def save_best_model(self, model_name):\n",
    "    save_dir=os.path.join(base_dir, 'models/sft/{:s}.pt'.format(model_name))\n",
    "    torch.save(self.tuning_logs['best_sft_model'].state_dict(), save_dir)\n",
    "    return\n",
    "  \n",
    "  def save_logs(self, name):\n",
    "    save_dir=os.path.join(base_dir, 'experiment logs/sft/{:s}.pkl'.format(name))\n",
    "    with open(save_dir, 'wb') as file:\n",
    "      pickle.dump(self.tuning_logs, file)\n",
    "  \n",
    "  def check_initial_scores(self):\n",
    "    trainer=SFTTrainer(self.data, self.max_gen_length, self.reward_model_name)\n",
    "    test_score=trainer.test()\n",
    "    val_score=trainer.validate()\n",
    "    return val_score, test_score\n",
    "\n",
    "  def tune(self, num_epochs_list, batch_size_list, lr_list, reg_list):\n",
    "    settings=self.get_settings(num_epochs_list, batch_size_list, lr_list, reg_list)\n",
    "    val_score, test_score=self.check_initial_scores()\n",
    "    print(\"val_score: {:.4f}, test_score: {:.4f}\".format(val_score, test_score))\n",
    "    outer_pbar=tqdm(desc=\"total sft tuning\", total=len(settings))\n",
    "    for setting in settings:\n",
    "      self.show_setting(setting)\n",
    "      trainer=SFTTrainer(self.data, self.max_gen_length, self.reward_model_name)\n",
    "      train_logs=trainer.train(*setting)\n",
    "      trainer.plot_train_logs()\n",
    "      self.tuning_logs['logs'][setting]=train_logs\n",
    "      if train_logs['test_score']>self.tuning_logs['best_test_score']:\n",
    "        self.tuning_logs['best_test_score']=train_logs['test_score']\n",
    "        self.tuning_logs['best_setting']=setting\n",
    "        self.tuning_logs['best_sft_model']=train_logs['best_sft_model']\n",
    "      outer_pbar.update(1)\n",
    "    outer_pbar.close()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data generated from rlhf_dataset.ipynb\n",
    "with open(os.path.join(base_dir, \"data/short_sft_data.pkl\"), 'rb') as file:\n",
    "  short_sft_data=pickle.load(file)\n",
    "with open(os.path.join(base_dir, \"data/sft_data.pkl\"), 'rb') as file:\n",
    "  sft_data=pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tune the SFT model using reward model trained earlier.\n",
    "tuner=SFTHyperparamTuner(short_sft_data, max_gen_length=200, reward_model_name=\"best_reward_model\")\n",
    "num_epochs_list=[2]\n",
    "batch_size_list=[8]\n",
    "lr_list=[1e-7, 1e-6, 1e-5, 1e-4, 1e-3]\n",
    "reg_list=[0]\n",
    "tuner.tune(num_epochs_list, batch_size_list, lr_list, reg_list)\n",
    "tuner.save_logs('sft_tuning_logs')\n",
    "tuner.save_best_model('best_sft_model')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bd385fe162c5ca0c84973b7dd5c518456272446b2b64e67c2a69f949ca7a1754"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
