{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#library for data processing\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from itertools import combinations\n",
    "import io\n",
    "import json\n",
    "import os\n",
    "import ast\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import transformers\n",
    "import accelerate\n",
    "from transformers import GPTNeoForCausalLM, GPT2Tokenizer, AdamW, get_scheduler, GPTNeoModel, GPT2LMHeadModel\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "#from torchrl.data import PrioritizedReplayBuffer, ReplayBuffer\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "import pickle\n",
    "import gym\n",
    "\n",
    "base_dir=\".\"\n",
    "\n",
    "accelerator=accelerate.Accelerator()\n",
    "device=accelerator.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing all datasets\n",
    "from dataset_classes.hf_dataset import HFDataset\n",
    "from dataset_classes.state_action_dataset import StateActionDataset\n",
    "from dataset_classes.ep_steps_dataset import EpStepsDataset\n",
    "from dataset_classes.eli5_and_hf_dataset import ELI5andHFDataset\n",
    "from dataset_classes.chosen_dataset import ChosenDataset\n",
    "from dataset_classes.eli5_dataset import ELI5Dataset\n",
    "from dataset_classes.instruct_dataset import InstructDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#125M version\n",
    "class GPTNeoRewardModel(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(GPTNeoRewardModel, self).__init__()\n",
    "    self.base=GPTNeoModel.from_pretrained('EleutherAI/gpt-neo-125M').to(device)\n",
    "    self.tokenizer = GPT2Tokenizer.from_pretrained('EleutherAI/gpt-neo-125M', bos_token='<|endoftext|>', eos_token='<|endoftext|>', padding_side=\"left\")\n",
    "    self.reward_head=nn.Linear(768, 1).to(device)\n",
    "    self.tokenizer.pad_token='<|endoftext|>'\n",
    "    \n",
    "  def forward(self, prompts):\n",
    "    #prompts include both state and action could be raw text of tensor\n",
    "    tokenized=self.tokenizer(prompts, return_tensors='pt', padding=True)\n",
    "    input_ids, attention_mask=tokenized.input_ids.to(device), tokenized.attention_mask.to(device)\n",
    "    last_hidden_states=self.base(input_ids, attention_mask=attention_mask).last_hidden_state\n",
    "    #use final hidden state's EOS embedding\n",
    "    eos_hidden_states=last_hidden_states[:,-1,:]\n",
    "    rewards=self.reward_head(eos_hidden_states)\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#10M version\n",
    "class GPTNeoRewardModel_10M(nn.Module):\n",
    "  def __init__(self, idx=0):\n",
    "    super(GPTNeoRewardModel_10M, self).__init__()\n",
    "    self.base=GPTNeoModel.from_pretrained('EleutherAI/gpt-neo-125M').to(device)\n",
    "    attention_layers=self.base.h\n",
    "    layer=attention_layers[idx]\n",
    "    self.base.h=nn.ModuleList([layer])\n",
    "    self.tokenizer = GPT2Tokenizer.from_pretrained('EleutherAI/gpt-neo-125M', bos_token='<|endoftext|>', eos_token='<|endoftext|>', padding_side=\"left\")\n",
    "    self.reward_head=nn.Linear(768, 1).to(device)\n",
    "    self.tokenizer.pad_token='<|endoftext|>'\n",
    "    \n",
    "  def forward(self, prompts):\n",
    "    #prompts include both state and action could be raw text of tensor\n",
    "    tokenized=self.tokenizer(prompts, return_tensors='pt', padding=True)\n",
    "    input_ids, attention_mask=tokenized.input_ids.to(device), tokenized.attention_mask.to(device)\n",
    "    last_hidden_states=self.base(input_ids, attention_mask=attention_mask).last_hidden_state\n",
    "    #use final hidden state's EOS embedding\n",
    "    eos_hidden_states=last_hidden_states[:,-1,:]\n",
    "    rewards=self.reward_head(eos_hidden_states)\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMTrainer():\n",
    "  def __init__(self, data, use_10M):\n",
    "    #baseline\n",
    "    if use_10M:\n",
    "      self.reward_model=GPTNeoRewardModel_10M()\n",
    "    else:\n",
    "      self.reward_model=GPTNeoRewardModel()\n",
    "\n",
    "    self.train_data=data['train_data']\n",
    "    self.val_data=data['val_data']\n",
    "    self.test_data=data['test_data']\n",
    "\n",
    "    #training logs\n",
    "    self.train_logs={\n",
    "      'loss_history': [],\n",
    "      'train_acc_history': [],\n",
    "      'val_acc_history': [],\n",
    "      'best_val_acc': 0,\n",
    "      'best_val_acc_epoch': 0,\n",
    "      'test_acc': 0,\n",
    "      'best_reward_model': self.reward_model\n",
    "    }\n",
    "  \n",
    "  def load_saved_model(self, model_name):\n",
    "    #load saved model into self.reward_model and best model in train logs.\n",
    "    model_path=os.path.join(base_dir, 'models/modified_rm/{:s}.pt'.format(model_name))\n",
    "    self.reward_model.load_state_dict(torch.load(model_path))\n",
    "    self.train_logs['best_reward_model'].load_state_dict(torch.load(model_path))\n",
    "    return\n",
    "  \n",
    "  def plot_train_logs(self):\n",
    "    loss_history=self.train_logs['loss_history']\n",
    "    iters=np.arange(1, len(loss_history)+1, 1)\n",
    "    train_acc_history=self.train_logs['train_acc_history']\n",
    "    n_epochs=np.arange(1, len(train_acc_history)+1, 1)\n",
    "    val_acc_history=self.train_logs['val_acc_history']\n",
    "    \n",
    "    plt.figure(figsize=(15,5))\n",
    "    #plot loss\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(iters, loss_history)\n",
    "    plt.title(\"Loss History\")\n",
    "    plt.xlabel(\"Iterations\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    #plot accuracy\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(n_epochs, train_acc_history, \"b-\", label=\"Train Accuracy\")\n",
    "    plt.plot(n_epochs, val_acc_history, \"r-\", label=\"Valid. Accuracy\")\n",
    "    #mark best val acc location\n",
    "    plt.plot(self.train_logs['best_val_acc_epoch'], self.train_logs['best_val_acc'], \"go\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.title(\"Train and Valid Loss\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "  def test(self):\n",
    "    #test with best model\n",
    "    test_loader=torch.utils.data.DataLoader(self.test_data, batch_size=32, shuffle=True)\n",
    "    avg_test_acc=0\n",
    "    best_model=self.train_logs['best_reward_model']\n",
    "    best_model.eval()\n",
    "    for iter in range(10):\n",
    "      test_acc=0\n",
    "      for idx, test_data in enumerate(test_loader):\n",
    "        chosens, rejecteds=test_data\n",
    "        rejected_list=[]\n",
    "        chosen_idx_list=[]\n",
    "        correct_count=0\n",
    "        for idx, rej in enumerate(rejecteds):\n",
    "          rejected=rej.split(\"<SEP>\")\n",
    "          rejected_list.extend(rejected)\n",
    "          rej_len=len(rejected_list)\n",
    "          chosen_idx_list.extend([idx for _ in range(rej_len)])\n",
    "\n",
    "        chosen_rewards=best_model(chosens)\n",
    "        rejected_rewards=best_model(rejected_list)\n",
    "        for rej_idx, rejected_reward in enumerate(rejected_rewards):\n",
    "          idx=chosen_idx_list[rej_idx]\n",
    "          chosen_reward=chosen_rewards[idx]\n",
    "          correct_count+=(chosen_reward>rejected_reward)\n",
    "        acc=correct_count/len(rejected_list)\n",
    "        test_acc=test_acc+(acc-test_acc)/(idx+1)\n",
    "      avg_test_acc=avg_test_acc+(test_acc-avg_test_acc)/(iter+1)\n",
    "    return avg_test_acc\n",
    "  \n",
    "  def validate(self):\n",
    "    self.reward_model.eval()\n",
    "    val_loader=torch.utils.data.DataLoader(self.val_data, batch_size=32, shuffle=True)\n",
    "    avg_val_acc=0\n",
    "    #check that obtained reward for chosen is higher than rejected\n",
    "    for iter in range(10):\n",
    "      val_acc=0\n",
    "      for idx, val_data in enumerate(val_loader):\n",
    "        chosens, rejecteds=val_data\n",
    "        rejected_list=[]\n",
    "        chosen_idx_list=[]\n",
    "        correct_count=0\n",
    "        for idx, rej in enumerate(rejecteds):\n",
    "          rejected=rej.split(\"<SEP>\")\n",
    "          rejected_list.extend(rejected)\n",
    "          rej_len=len(rejected_list)\n",
    "          chosen_idx_list.extend([idx for _ in range(rej_len)])\n",
    "\n",
    "        chosen_rewards=self.reward_model(chosens)\n",
    "        rejected_rewards=self.reward_model(rejected_list)\n",
    "        for rej_idx, rejected_reward in enumerate(rejected_rewards):\n",
    "          idx=chosen_idx_list[rej_idx]\n",
    "          chosen_reward=chosen_rewards[idx]\n",
    "          correct_count+=(chosen_reward>rejected_reward)\n",
    "        acc=correct_count/len(rejected_list)\n",
    "        val_acc=val_acc+(acc-val_acc)/(idx+1)\n",
    "      avg_val_acc=avg_val_acc+(val_acc-avg_val_acc)/(iter+1)\n",
    "    return avg_val_acc\n",
    "  \n",
    "  def get_loss(self, chosens, rejecteds):\n",
    "    #there could be multiple rejected sentences: compatible for both Anthropic HF dset & ELI5 dataset.\n",
    "    batch_size=len(chosens)\n",
    "    sigmoid=nn.Sigmoid()\n",
    "    #list used to handle multiple rejected sentences\n",
    "    rejected_list=[]\n",
    "    chosen_idx_list=[]\n",
    "    correct_count=0\n",
    "    #get list of rejected phrases\n",
    "    for idx, rej in enumerate(rejecteds):\n",
    "      rejected=rej.split(\"<SEP>\")\n",
    "      rejected_list.extend(rejected)\n",
    "      rej_len=len(rejected_list)\n",
    "      chosen_idx_list.extend([idx for _ in range(rej_len)])\n",
    "\n",
    "    chosen_rewards=self.reward_model(chosens)\n",
    "    rejected_rewards=self.reward_model(rejected_list)\n",
    "    loss=torch.FloatTensor([0]).to(device)\n",
    "    for rej_idx, rejected_reward in enumerate(rejected_rewards):\n",
    "      idx=chosen_idx_list[rej_idx]\n",
    "      chosen_reward=chosen_rewards[idx]\n",
    "      correct_count+=(chosen_reward>rejected_reward)\n",
    "      loss=loss-torch.log(sigmoid(chosen_reward-rejected_reward))\n",
    "    loss=loss/len(rejected_list)\n",
    "    accuracy=correct_count/len(rejected_list)\n",
    "    return loss, accuracy\n",
    "  \n",
    "  def train(self, num_epochs, batch_size, lr, reg):\n",
    "    train_loader=torch.utils.data.DataLoader(self.train_data, batch_size=batch_size, shuffle=True)\n",
    "    rm_optimizer=optim.AdamW(self.reward_model.parameters(), lr=lr, weight_decay=reg)\n",
    "    \n",
    "    setting=(num_epochs, batch_size, lr, reg)\n",
    "    pbar=tqdm(desc=\"RM training\", total=num_epochs*len(self.train_data), leave=False)\n",
    "    for epoch in range(1, num_epochs+1):\n",
    "      train_acc=0\n",
    "      #print(\"Epoch {:d}\".format(epoch))\n",
    "      for batch_idx, batch_data in enumerate(train_loader):\n",
    "        chosens, rejecteds=batch_data\n",
    "        self.reward_model.train()\n",
    "        loss, acc=self.get_loss(chosens, rejecteds)\n",
    "        self.train_logs['loss_history'].append(loss.item())\n",
    "        #print(self.train_logs['loss_history'])\n",
    "\n",
    "        #weight update\n",
    "        rm_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        rm_optimizer.step()\n",
    "\n",
    "        pbar.update(batch_size)\n",
    "\n",
    "        #train acc update\n",
    "        train_acc=train_acc+(acc-train_acc)/(batch_idx+1)\n",
    "        #print(loss)\n",
    "      #validate and log validation data\n",
    "      val_acc=self.validate()\n",
    "      #update train logs\n",
    "      self.train_logs['train_acc_history'].append(train_acc.item())\n",
    "      self.train_logs['val_acc_history'].append(val_acc.item())\n",
    "      if val_acc>self.train_logs['best_val_acc']:\n",
    "        self.train_logs['best_val_acc']=val_acc.item()\n",
    "        self.train_logs['best_val_acc_epoch']=epoch\n",
    "        self.train_logs['best_reward_model']=self.reward_model\n",
    "    self.train_logs['test_acc']=self.test().item()\n",
    "    #plot\n",
    "    self.plot_train_logs()\n",
    "    pbar.close()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMHyperparamTuningModule():\n",
    "  def __init__(self, data):\n",
    "    self.data=data\n",
    "\n",
    "    #recording tuning data\n",
    "    self.best_test_acc=0\n",
    "    self.best_reward_model=None\n",
    "\n",
    "    self.tuning_logs={\n",
    "        'logs': {},\n",
    "        'best_setting': (),\n",
    "        'best_test_acc': 0,\n",
    "        'best_reward_model': None\n",
    "    }\n",
    "  \n",
    "  def get_settings(self, n_epochs_list, batch_size_list, lr_list, reg_list):\n",
    "    settings=[]\n",
    "    for n_epoch in n_epochs_list:\n",
    "      for batch_size in batch_size_list:\n",
    "        for lr in lr_list:\n",
    "          for reg in reg_list:\n",
    "            setting=(n_epoch, batch_size, lr, reg)\n",
    "            settings.append(setting)\n",
    "    return settings\n",
    "  \n",
    "  def show_setting(self, setting):\n",
    "    print(\"Setting: n_epochs={:d}, batch_size={:d}, lr={:s}, reg={:s}\".format(setting[0], setting[1], str(setting[2]), str(setting[3])))\n",
    "    return\n",
    "  \n",
    "  def save_best_model(self, name):\n",
    "    rm_dir=os.path.join(base_dir, 'models/modified_rm/{:s}.pt'.format(name))\n",
    "    torch.save(self.tuning_logs['best_reward_model'].state_dict(), rm_dir)\n",
    "  \n",
    "  def save_tuning_logs(self, name):\n",
    "    log_dir=os.path.join(base_dir, 'experiment logs/modified_rm/{:s}.pkl'.format(name))\n",
    "    with open(log_dir, 'wb') as file:\n",
    "      pickle.dump(self.tuning_logs, file)\n",
    "    return\n",
    "  \n",
    "  def check_initial_accuracy(self, use_10M):\n",
    "    trainer=RMTrainer(self.data, use_10M)\n",
    "    test_acc=trainer.test()\n",
    "    val_acc=trainer.validate()\n",
    "    return val_acc, test_acc\n",
    "\n",
    "  def tune(self, use_10M, n_epochs_list, batch_size_list, lr_list, reg_list):\n",
    "    settings=self.get_settings(n_epochs_list, batch_size_list, lr_list, reg_list)\n",
    "    #check initial accuracy to check improvements\n",
    "    val_acc, test_acc=self.check_initial_accuracy(use_10M)\n",
    "    print(\"val_acc: {:.4f}, test_acc: {:.4f}\".format(val_acc, test_acc))\n",
    "    outer_pbar=tqdm(desc=\"Total tuning\", total=len(settings))\n",
    "    for setting in settings:\n",
    "      self.show_setting(setting)\n",
    "      trainer=RMTrainer(self.data, use_10M=use_10M)\n",
    "      trainer.train(*setting)\n",
    "      self.tuning_logs['logs'][setting]=trainer.train_logs\n",
    "      if trainer.train_logs['test_acc']>self.tuning_logs['best_test_acc']:\n",
    "        self.tuning_logs['best_setting']=setting\n",
    "        self.tuning_logs['best_test_acc']=trainer.train_logs['test_acc']\n",
    "        self.tuning_logs['best_reward_model']=trainer.train_logs['best_reward_model']\n",
    "      outer_pbar.update(1)\n",
    "    outer_pbar.close()\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data from rlhf_dataset.ipynb\n",
    "#load raw data for rm training => short version to mitigate memory issues.\n",
    "with open(os.path.join(base_dir, 'data/short_rm_data.pkl'), 'rb') as file:\n",
    "  rm_data=pickle.load(file)\n",
    "#load ELI5 data\n",
    "with open(os.path.join(base_dir, 'data/short_eli5_data.pkl'), 'rb') as file:\n",
    "  eli5_data=pickle.load(file)\n",
    "#load ELI5 and Anthropic HF data\n",
    "with open(os.path.join(base_dir, 'data/short_eli5_and_hf_data.pkl'), 'rb') as file:\n",
    "  eli5_and_hf_data=pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tune the model\n",
    "tuner=RMHyperparamTuningModule(rm_data)\n",
    "n_epochs_list=[10]\n",
    "batch_size_list=[8]\n",
    "lr_list=[1.414e-6, 2e-6, 2.828e-6, 4e-6, 5.657e-6, 8e-6, 11.313e-6]\n",
    "reg_list=[1e-5] #lr-decay rate\n",
    "tuner.tune(True, n_epochs_list, batch_size_list, lr_list, reg_list)\n",
    "tuner.save_tuning_logs(\"tuning_logs\")\n",
    "tuner.save_best_model('best_reward_model')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bd385fe162c5ca0c84973b7dd5c518456272446b2b64e67c2a69f949ca7a1754"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
